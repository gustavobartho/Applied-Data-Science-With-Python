{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Text Classification**\n",
    "\n",
    "## **Supervised Classification**\n",
    "Learn a **classification model** on properties (\"features\") and their importance (\"weights\") from labeled instances.\n",
    "* $X$: Set of attributes or features $\\{x_1, x_2, ..., x_n\\}$\n",
    "* $y$: A \"class\" label from the label set $Y = \\{y_1, y_2, ..., y_k\\}$\n",
    "\n",
    "#### **Classification Paradigms**\n",
    "* **Binary Classification**: When there are only two possible classes; $|Y| = 2$\n",
    "* **Multi-class Classification**: When there are more than two possible classes; $|Y| > 2$\n",
    "\n",
    "#### **Questions to ask in Supervised Learning**\n",
    "* Training phase:\n",
    "    * What are the features? How to represent them?\n",
    "    * What is the classification model/algorithm?\n",
    "    * What are the model parameters?\n",
    "* Inference phase:\n",
    "    * What is the expected performance? What is a good measure?\n",
    "\n",
    "#### **Types of textual features**\n",
    "* Words\n",
    "    * By far the most common class of features\n",
    "    * Handling commonly-occurring words: Stop words\n",
    "    * Normalization: Make lower case vs. leave as-is\n",
    "    * Stemming / Lemmatization\n",
    "* Characteristics of words: Capitalization\n",
    "* Parts of speech of words in a sentence\n",
    "* Grammatical structure, sentence parsing\n",
    "* Grouping words of similar meaning, semantics\n",
    "* Depending on the classification task, features may come from inside words and word sequences\n",
    "    * bigrams, trigrams, n-grams (e.g. White House)\n",
    "    * character sub-sequences in words (e.g. \"ing\", \"ion\", ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## **Naive Bayes Classifiers**\n",
    "Updates the likelihood of the class given new information\n",
    "* $Posterior\\ probability = {{Prior\\ probability \\times Likelihood} \\over Evidence}$\n",
    "\n",
    "* $Pr(y|X) = {{Pr(y) \\times Pr(X|y)}\\over Pr(X)}$\n",
    "\n",
    "* $y^* = argmax_y\\ Pr(y|x) = argmax_y\\ Pr(y) \\times Pr(X|y)$\n",
    "\n",
    "* **Naive assumption**: Given the class label, features are assumed to be independent of each other\n",
    "\n",
    "* $\\displaystyle y^* = argmax_y\\ Pr(y|X) = argmax_y\\ Pr(y) \\times \\prod_{n}^{i=1} Pr(x_i|y)$\n",
    "\n",
    "* The predicted label $y$ is the $y$ that maximizes, the computation of probability of $y$ given $X$. Which is computed using Bayes Rule as probability of $y$, that is, the prior times $n$ independent products of individual features given $y$. So that's the likelihood of probability of $X$ given $y$.\n",
    "\n",
    "#### **What are the parameters?**\n",
    "* **Prior probabilities**: $Pr(y)$ for all $y$ in $Y$\n",
    "    * Count the number of instances of each class\n",
    "    * If there are $N$ instancesin all, and $n$ out of those are labeled as class $y$ then $Pr(y) = {n \\over N}$\n",
    "\n",
    "* **Likelihood**: $Pr(x_i|y)$ for all features $x_i$ and labels $y$ in $Y$\n",
    "    * Count how many times feature $x_i$ appears in instances labeled as class $y$\n",
    "    * If there are $p$ instances of class $y$, and $x_i$ appears in $k$ of those then $Pr(x_i|y) = {k \\over p}$\n",
    "\n",
    "#### **Smoothing**\n",
    "* What happens if $Pr(x_i|y) = 0$ ?\n",
    "    * If feature $x_i$ never occurs in documents labeled as $y$, the posterior probability $Pr(y|x_i)$ will be $0$\n",
    "* **Laplace smoothing** or **Additive smoothing**: Add a dummy count so every word start with a count of $1$ for every class\n",
    "    * $Pr(x_i|y) = {{k + 1}\\over{p + n}}$ where $n$ is the number of features\n",
    "\n",
    "#### **Important Concepts**\n",
    "* Naive Bayes is a probabilistic model\n",
    "* It is called naive because it assumes features are independent of each other, given the class label\n",
    "* For text classification problems, Naive Bayes models typically provide very strong baselines \n",
    "* Simple model, easy to learn parameters\n",
    "\n",
    "#### **Naive Bayes variations**\n",
    "* **Multinomial Naive Bayes**\n",
    "    * Data follows a multinomial distribution\n",
    "    * Each feature value is a count (e.g. word occurrence count, TF-IDF weighting, ...)\n",
    "* **Bernoulli naive Bayes**\n",
    "    * Data follows a multivariate Bernoulli distribution\n",
    "    * Each feature is binary (e.g. word is present / absent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN DATA:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 60000 entries, 0 to 119981\n",
      "Data columns (total 3 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   Class Index  60000 non-null  int64 \n",
      " 1   Title        60000 non-null  object\n",
      " 2   Description  60000 non-null  object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 1.8+ MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TEST DATA:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 3800 entries, 0 to 7599\n",
      "Data columns (total 3 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   Class Index  3800 non-null   int64 \n",
      " 1   Title        3800 non-null   object\n",
      " 2   Description  3800 non-null   object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 118.8+ KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer, HashingVectorizer\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "train_df = pd.read_csv('../datasets/News_Classification/train.csv')\n",
    "test_df = pd.read_csv('../datasets/News_Classification/test.csv')\n",
    "\n",
    "# keeps only the Business(3) and Sci/Tech(4) news\n",
    "train_df = train_df[train_df['Class Index'].isin([3, 4])]\n",
    "test_df = test_df[test_df['Class Index'].isin([3, 4])]\n",
    "\n",
    "# transforms labels 3 -> 0 and 4 -> 1\n",
    "replace_labels = {3: 0, 4: 1}\n",
    "train_df['Class Index'].replace(replace_labels, inplace=True)\n",
    "test_df['Class Index'].replace(replace_labels, inplace=True)\n",
    "\n",
    "# concatenate title and description in only one text\n",
    "train_df['Description'] = train_df.apply(lambda x: ' '.join([str(x['Title']), str(x['Description'])]), axis=1)\n",
    "test_df['Description'] = test_df.apply(lambda x: ' '.join([str(x['Title']), str(x['Description'])]), axis=1)\n",
    "\n",
    "# gets X and y data\n",
    "X_train, y_train = train_df['Description'].to_list(), train_df['Class Index'].to_list()\n",
    "X_test, y_test = test_df['Description'].to_list(), test_df['Class Index'].to_list()\n",
    "\n",
    "print('TRAIN DATA:')\n",
    "display(train_df.info())\n",
    "\n",
    "print('\\nTEST DATA:')\n",
    "display(test_df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8721052631578947\n"
     ]
    }
   ],
   "source": [
    "count_vectorizer = CountVectorizer(stop_words='english', max_features=1500, ngram_range=(1, 3))\n",
    "X_train_count = count_vectorizer.fit_transform(X_train)\n",
    "X_test_count = count_vectorizer.transform(X_test)\n",
    "\n",
    "multNB = MultinomialNB()\n",
    "multNB.fit(X_train_count, y_train)\n",
    "\n",
    "y_pred_mult = multNB.predict(X_test_count)\n",
    "print(f1_score(y_test, y_pred_mult, average='micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8623684210526316\n"
     ]
    }
   ],
   "source": [
    "hash_vectorizer = HashingVectorizer(stop_words='english', binary=True, n_features=1500,  ngram_range=(1, 3))\n",
    "X_train_hash = count_vectorizer.fit_transform(X_train)\n",
    "X_test_hash = count_vectorizer.transform(X_test)\n",
    "\n",
    "berNB = BernoulliNB()\n",
    "berNB.fit(X_train_hash, y_train)\n",
    "\n",
    "y_pred_ber = berNB.predict(X_test_hash)\n",
    "print(f1_score(y_test, y_pred_ber, average='micro'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-science",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
